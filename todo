1>TODO: 
- !SESS and CAMERAS to attribution benchmark
- erick pipeline diagram
- how many RUNS: elp 2,elp_with_scales_normalized 2,gp 2,with gp 2, unweighted 2,multiple seeds 3,3,3 = 16
- run on other methods shown in the elp paper (grad_cam etc): EP, gradient 2, guided 2, grad cam 2,rise 2 = 8
- keep an eye out for negative correlation 
--------------------------------------------------------------------------
    6>DELETION
    - deletion game
    - metricsdatahandler
    - elp masking to deletion
    - we will havetorerun deletion on all methods on2 datasets and 2 arch
    - average across percentages or not in deletion? YES to keep it consistent across number of thresholds, it should be average? should be in icdmpaper
    - ( pmasked -p) or (p - pmasked): we did see negative scores -> we take difference (was that for deletion?). - should also store areas in deletion
    - average deletion
    - separate insertion and deletion percentages
    - what were the percentages for gpnn deletion evaluation, or do i run everything again?
--------------------------------------------------------------------------
        - MULTI:sharpness annealing
        - svhn in torchray. run pointing. find pretrained
        - finish deletion
        - mnist for detection or small datasets for detection. 
        - cifar in torchray. dont run pointing
---------------------------------------------------------------------------
PAPER LINKS
- ijcai primary seconday, abstract
- https://cmt3.research.microsoft.com/IJCAI2024/Submission/Index#
- IJCNN
https://edas.info/N31614?c=31614
https://edas.info/newPaper.php?c=31628&track=120748
--------------------------------------------------------------------------
-------------------------------------------------------------------------
2>DONE:
- pause only if certain debugging flags are set
- make a workflow file for the steps required
- automatically creatae a folder with the image whose attribution we require and attributions produced by all methods
- rclone scripts for syncing to remote
- run elp+gp on 117,118
- correct all resutls
- rerun gradcam on initial
- run summary on finished results: it is still giving same answer for difficult and non difficult: !might mess things up --> run it on the cheapest method (gradcam)
- create done lists
- create new 114,116 scripts to run the same metod in parallel
- add download to setup 
- compare results with compiled model
- check methodname for save?
- check if the visualizers are working correctly: gp, unweighted
- remove axis from scatter plot
- fix opencv error  apt-get update && apt-get install ffmpeg libsm6 libxext6  -y
- generate sanity for extremal perturbation
- lsxz,readxz
- spearman rank corr on rise data
- check_n_done
- message saumya
- run method on a chosen list of imroots
- see if gradcam and rise results can be read by explore_results
- elp convert download scripts to python: create alias
- try moving to second vast system
- elp run-scripts
- function to check all uploads to a host: write a bash function that takes an identifier string called instance_name as input and then echos it
- script to login to all instances and check their tmux sessions
- in generate scripts add save_detailed_results
- after backup kill 112b
- instead of running per method, meker per instance files
- check benchmark,and deterministic
- generate run scrips for elp 50 , and run
- jupyter password not workin: restarting workds
- code server password not working: fixed by copying config yaml
- vast copy syntax
- add insertion t deletion
- deletion getmodel etc
- vast upload benchmark directory
- how to send instance info 
- upload vutils
- deletion inference mode
- gp saliency
- gp saliency gradient based
- gp saliency meshgrid
- check if weights are normalized correctly in normalized
- verify unw and normalized samplers produce same traces
- verify effect of rng patchsampler
- vim utoindent
- rclone scripts for downloading cam-benchmark stuff 
- rclone scripts foruploading torchray
- install rclone
- dutils hack early loop
- dutils glob
- get_model
- cam_benchmark read imagenet
- imagenet 5000
- get_dataset
- get_transform
- dataset.images in continue
- imagenet localization parser: https://github.com/tumble-weed/GPNN-CAM/blob/1e1801b4a61e288971180a7102687efaf805a1b4/imagenet_localization_parser.py
- kill third machine before sleeping
- use_donefilelist
- continue filelist
- fileorder
- rng
- remove keys from vutils
- computation of point in variants vs attribution-benchmark
- return from extremal_perturbation
- compiled model in attribution_benchmark
- check if continue works when start,end and some images are present
- add file name to dutils hack, note
- continue_ start and end
- pascal class images etc in torchray
- add tensor_to_numpy to dutils
- who copies the git aliases
- perturbation to opts
- create environment
- install dutils
conda clean --all
- dutils:
scikit-image
matplotlib
ipdb
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
--------------------------------------------------------------------------
7>CHATGPT
- vim_and_commit: write a bash function vim_and_commit that takes a filename as input, cd into the containing directory for that file, opens the file with vim. upon closing, it adds and commits the file using the basename of the file as the commit message.
- python: given a module name modulename, import it from a directory called wrappers_for_torchray if a file called f"{modulename}.py" exists in wrappers_for_torchray
- subclass ipdb so that set_trace can take an enabled flag and an environment variable name. if the enabled flag is true or the environment variable is set the breakpoint is set
- how to unload an alias
- multiple copy buffers in vim
------------------------------------------------------------------------
4>LATER:
- tight in dutils img_save
- run imagenet on elp paper methods (!might not be better than eelp)
- cam-benchmark
- visualize results for multiple methods ( cherry picking code?) (explore results) -- lowest sum of pointing
- check if libracam model results compatible with torchray
- after
- wrappers for benchmark in torchray
- training cifar and mnist, emnist networks
- reading gpnn attribution files and running deletion metric on them 

- window sampling density
- mask history

- write about spearman corr/read up on it
- vim syntax highlighting
- cifar and minst in torchray
- $VUTILS
- equalized sampling
- elp scripts
- elp vast-scripts
- copy download voc to cam_benchmark as well
- coco class images in torchray
- add as_class_names etc to coco
.......................................
- add scripts for rclone
.......................................
- list files in directory using git
- copy git files to gdrive
- copy files at frequent interval
- make service to list all directories with .git in them and copy those files

-------------------------------------------------------------------------
3>RECHECK:
- resize for imagenet
- automatically create table from finished results
- report per class?
- correct extremal variants
- sumarize all methods
- imagenet results
- generate sanity of gp
- run_on_single_image_backend: should be to choose the image, while keeping the front end clean. should go through variants rather than directly calling oct17
- variants
- replace oct17 by variants in run_on_image2
- run_on_single_image 2
- where to attach visualizer ( in run_image file)

- commit everything in 112b before shifting
- onstart vast-utils in workspace and final vast-utils
- copy stuff to vast-112-original
- run sanity
#- visualize attribution at the end of elp
#- visualize some crops
#- hack for visualization
#- sampling density for gaussian process windows
#- visualiz gp scatter
#- visualize gp modeled prob

- verify effect of rng gpsampler
- vmin and vmax in dutils
- save sanity checks and rankcorr to metrics dir
- why is rank corr negative: check when running on multiple seeds
- load results according to explore_results for spearman
- spearman correlation
- load saved results using results data handler
- center did not get saved
- move results datahandler to cam-benchmark <only after you've figured out loading of results etc..>
- reload in dutils
- rng in savename
- sanity checks
- crop sampling: limits should be int or floor or ceil?
- replace areas_torch with areas_scipy everywhere
- multi save
- wrapper for multi
- summary
- correct classes of saved resutls
- elp convert upload scripts to python
- patch_sampler
- create run scripts
-_oct17
- explore results
- change location of compiled in multithresh
- resultsdatahandler
- add optional saving to attribution_benchmark
- fix voc error
- download pascal dataset
- cam_benchmark models
- librecam models
-saver
-change tracker
- abspath in dutils
- 3 step softmax fixed window sizes
- init_mask_logit_mag
------------------------------------------------------------------------
5>MAYBE:
- visualize multi max of smooth and gradient side by side
------------------------------------------------------------------------
7>DANGERS:
gp saliency ight be bettwer
unw or normalized miht be better
elp-gp might not show improvements
the correlation might not be high enough
