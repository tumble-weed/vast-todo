1>TODO:
- see if gradcam and rise results can be read by explore_results
- visualize results for multiple methods ( cherry picking code?) (explore results) -- lowest sum of pointing
- message saumya
- check if libracam model results compatible with torchray
.......................................
- how many RUNS: elp 2,elp_with_scales_normalized 2,gp 2,with gp 2, unweighted 2,multiple seeds 3,3,3 = 16
- run on other methods shown in the elp paper (grad_cam etc): EP, gradient 2, guided 2, grad cam 2,rise 2 = 8
- !SESS and CAMERAS to attribution benchmark
- erick pipeline diagram
- run method on a chosen list of imroots
--------------------------------------------------------------------------
- variants
- replace oct17 by variants in run_on_image2
- run_on_single_image_backend: should be to choose the image, while keeping the front end clean. should go through variants rather than directly calling oct17
- rn_on_single_image 2
- where to attach visualizer ( in run_image file)
--------------------------------------------------------------------------
- cam-benchmark
- deletion game
- metricsdatahandler
.......................................
- MULTI:sharpness annealing
.......................................
2>DONE:
- elp convert download scripts to python: create alias
- try moving to second vast system
- elp run-scripts
- function to check all uploads to a host: write a bash function that takes an identifier string called instance_name as input and then echos it
- script to login to all instances and check their tmux sessions
- in generate scripts add save_detailed_results
- after backup kill 112b
- instead of running per method, meker per instance files
- check benchmark,and deterministic
- generate run scrips for elp 50 , and run
- jupyter password not workin: restarting workds
- code server password not working: fixed by copying config yaml
- vast copy syntax
- add insertion t deletion
- deletion getmodel etc
- vast upload benchmark directory
- how to send instance info 
- upload vutils
- deletion inference mode
- gp saliency
- gp saliency gradient based
- gp saliency meshgrid
- check if weights are normalized correctly in normalized
- verify unw and normalized samplers produce same traces
- verify effect of rng patchsampler
- vim utoindent
- rclone scripts for downloading cam-benchmark stuff 
- rclone scripts foruploading torchray
- install rclone
- dutils hack early loop
- dutils glob
- get_model
- cam_benchmark read imagenet
- imagenet 5000
- get_dataset
- get_transform
- dataset.images in continue
- imagenet localization parser: https://github.com/tumble-weed/GPNN-CAM/blob/1e1801b4a61e288971180a7102687efaf805a1b4/imagenet_localization_parser.py
- kill third machine before sleeping
- use_donefilelist
- continue filelist
- fileorder
- rng
- remove keys from vutils
- computation of point in variants vs attribution-benchmark
- return from extremal_perturbation
- compiled model in attribution_benchmark
- check if continue works when start,end and some images are present
- add file name to dutils hack, note
- continue_ start and end
- pascal class images etc in torchray
- add tensor_to_numpy to dutils
- who copies the git aliases
- perturbation to opts
- create environment
- install dutils
conda clean --all
- dutils:
scikit-image
matplotlib
ipdb
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
--------------------------------------------------------------------------
6>DELETION
- elp masking to deletion
- we will havetorerun deletion on all methods on2 datasets and 2 arch
- average across percentages or not in deletion? YES to keep it consistent across number of thresholds, it should be average? should be in icdmpaper
- ( pmasked -p) or (p - pmasked): we did see negative scores -> we take difference (was that for deletion?). - should also store areas in deletion
- average deletion
- separate insertion and deletion percentages
- what were the percentages for gpnn deletion evaluation, or do i run everything again?
--------------------------------------------------------------------------
7>CHATGPT
- vim_and_commit: write a bash function vim_and_commit that takes a filename as input, cd into the containing directory for that file, opens the file with vim. upon closing, it adds and commits the file using the basename of the file as the commit message.
- python: given a module name modulename, import it from a directory called wrappers_for_torchray if a file called f"{modulename}.py" exists in wrappers_for_torchray
- subclass ipdb so that set_trace can take an enabled flag and an environment variable name. if the enabled flag is true or the environment variable is set the breakpoint is set
- how to unload an alias
------------------------------------------------------------------------
4>LATER:
- after
- wrappers for benchmark in torchray
- training cifar and mnist, emnist networks
- reading gpnn attribution files and running deletion metric on them 

- window sampling density
- mask history

- write about spearman corr/read up on it
- vim syntax highlighting
- lsxz,readxz
- cifar and minst in torchray
- $VUTILS
- equalized sampling
- elp scripts
- elp vast-scripts
- copy download voc to cam_benchmark as well
- coco class images in torchray
- add as_class_names etc to coco
.......................................
- add scripts for rclone
.......................................
- list files in directory using git
- copy git files to gdrive
- copy files at frequent interval
- make service to list all directories with .git in them and copy those files

-------------------------------------------------------------------------
3>RECHECK:
- commit everything in 112b before shifting
- onstart vast-utils in workspace and final vast-utils
- copy stuff to vast-112-original
- run sanity
#- visualize attribution at the end of elp
#- visualize some crops
#- hack for visualization
#- sampling density for gaussian process windows
#- visualiz gp scatter
#- visualize gp modeled prob

- verify effect of rng gpsampler
- vmin and vmax in dutils
- save sanity checks and rankcorr to metrics dir
- why is rank corr negative: check when running on multiple seeds
- load results according to explore_results for spearman
- spearman correlation
- load saved results using results data handler
- center did not get saved
- move results datahandler to cam-benchmark <only after you've figured out loading of results etc..>
- reload in dutils
- rng in savename
- sanity checks
- crop sampling: limits should be int or floor or ceil?
- replace areas_torch with areas_scipy everywhere
- multi save
- wrapper for multi
- summary
- correct classes of saved resutls
- elp convert upload scripts to python
- patch_sampler
- create run scripts
-_oct17
- explore results
- change location of compiled in multithresh
- resultsdatahandler
- add optional saving to attribution_benchmark
- fix voc error
- download pascal dataset
- cam_benchmark models
- librecam models
-saver
-change tracker
- abspath in dutils
- 3 step softmax fixed window sizes
- init_mask_logit_mag
------------------------------------------------------------------------
5>MAYBE:
- visualize multi max of smooth and gradient side by side
------------------------------------------------------------------------
7>DANGERS:
gp saliency ight be bettwer
unw or normalized miht be better
elp-gp might not show improvements
the correlation might not be high enough
